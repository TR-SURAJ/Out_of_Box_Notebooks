{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Identify Columns That Contain a Single Value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import urlopen\n",
    "from numpy import loadtxt\n",
    "from numpy import unique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 238\n",
      "1 297\n",
      "2 927\n",
      "3 933\n",
      "4 179\n",
      "5 375\n",
      "6 820\n",
      "7 618\n",
      "8 561\n",
      "9 57\n",
      "10 577\n",
      "11 59\n",
      "12 73\n",
      "13 107\n",
      "14 53\n",
      "15 91\n",
      "16 893\n",
      "17 810\n",
      "18 170\n",
      "19 53\n",
      "20 68\n",
      "21 9\n",
      "22 1\n",
      "23 92\n",
      "24 9\n",
      "25 8\n",
      "26 9\n",
      "27 308\n",
      "28 447\n",
      "29 392\n",
      "30 107\n",
      "31 42\n",
      "32 4\n",
      "33 45\n",
      "34 141\n",
      "35 110\n",
      "36 3\n",
      "37 758\n",
      "38 9\n",
      "39 9\n",
      "40 388\n",
      "41 220\n",
      "42 644\n",
      "43 649\n",
      "44 499\n",
      "45 2\n",
      "46 937\n",
      "47 169\n",
      "48 286\n",
      "49 2\n"
     ]
    }
   ],
   "source": [
    "path = 'https://raw.githubusercontent.com/jbrownlee/Datasets/master/oil-spill.csv'\n",
    "data = loadtxt(urlopen(path), delimiter=',')\n",
    "for i in range(data.shape[1]):\n",
    "    print(i,len(unique(data[:,i])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that column index 22 only has a single value and should be removed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can also use nunique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0     238\n",
      "1     297\n",
      "2     927\n",
      "3     933\n",
      "4     179\n",
      "5     375\n",
      "6     820\n",
      "7     618\n",
      "8     561\n",
      "9      57\n",
      "10    577\n",
      "11     59\n",
      "12     73\n",
      "13    107\n",
      "14     53\n",
      "15     91\n",
      "16    893\n",
      "17    810\n",
      "18    170\n",
      "19     53\n",
      "20     68\n",
      "21      9\n",
      "22      1\n",
      "23     92\n",
      "24      9\n",
      "25      8\n",
      "26      9\n",
      "27    308\n",
      "28    447\n",
      "29    392\n",
      "30    107\n",
      "31     42\n",
      "32      4\n",
      "33     45\n",
      "34    141\n",
      "35    110\n",
      "36      3\n",
      "37    758\n",
      "38      9\n",
      "39      9\n",
      "40    388\n",
      "41    220\n",
      "42    644\n",
      "43    649\n",
      "44    499\n",
      "45      2\n",
      "46    937\n",
      "47    169\n",
      "48    286\n",
      "49      2\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "from pandas import read_csv\n",
    "path = 'https://raw.githubusercontent.com/jbrownlee/Datasets/master/oil-spill.csv'\n",
    "df = read_csv(path, header=None)\n",
    "print(df.nunique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Delete Columns That Contain a Single Value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(937, 49)\n"
     ]
    }
   ],
   "source": [
    "colval = df.nunique()\n",
    "singlevalcol = [i for i,v in enumerate(colval) if v==1]\n",
    "df.drop(singlevalcol,axis=1,inplace=True)\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>40</th>\n",
       "      <th>41</th>\n",
       "      <th>42</th>\n",
       "      <th>43</th>\n",
       "      <th>44</th>\n",
       "      <th>45</th>\n",
       "      <th>46</th>\n",
       "      <th>47</th>\n",
       "      <th>48</th>\n",
       "      <th>49</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2558</td>\n",
       "      <td>1506.09</td>\n",
       "      <td>456.63</td>\n",
       "      <td>90</td>\n",
       "      <td>6395000.0</td>\n",
       "      <td>40.88</td>\n",
       "      <td>7.89</td>\n",
       "      <td>29780.0</td>\n",
       "      <td>0.19</td>\n",
       "      <td>...</td>\n",
       "      <td>2850.00</td>\n",
       "      <td>1000.00</td>\n",
       "      <td>763.16</td>\n",
       "      <td>135.46</td>\n",
       "      <td>3.73</td>\n",
       "      <td>0</td>\n",
       "      <td>33243.19</td>\n",
       "      <td>65.74</td>\n",
       "      <td>7.95</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>22325</td>\n",
       "      <td>79.11</td>\n",
       "      <td>841.03</td>\n",
       "      <td>180</td>\n",
       "      <td>55812500.0</td>\n",
       "      <td>51.11</td>\n",
       "      <td>1.21</td>\n",
       "      <td>61900.0</td>\n",
       "      <td>0.02</td>\n",
       "      <td>...</td>\n",
       "      <td>5750.00</td>\n",
       "      <td>11500.00</td>\n",
       "      <td>9593.48</td>\n",
       "      <td>1648.80</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0</td>\n",
       "      <td>51572.04</td>\n",
       "      <td>65.73</td>\n",
       "      <td>6.26</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>115</td>\n",
       "      <td>1449.85</td>\n",
       "      <td>608.43</td>\n",
       "      <td>88</td>\n",
       "      <td>287500.0</td>\n",
       "      <td>40.42</td>\n",
       "      <td>7.34</td>\n",
       "      <td>3340.0</td>\n",
       "      <td>0.18</td>\n",
       "      <td>...</td>\n",
       "      <td>1400.00</td>\n",
       "      <td>250.00</td>\n",
       "      <td>150.00</td>\n",
       "      <td>45.13</td>\n",
       "      <td>9.33</td>\n",
       "      <td>1</td>\n",
       "      <td>31692.84</td>\n",
       "      <td>65.81</td>\n",
       "      <td>7.84</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1201</td>\n",
       "      <td>1562.53</td>\n",
       "      <td>295.65</td>\n",
       "      <td>66</td>\n",
       "      <td>3002500.0</td>\n",
       "      <td>42.40</td>\n",
       "      <td>7.97</td>\n",
       "      <td>18030.0</td>\n",
       "      <td>0.19</td>\n",
       "      <td>...</td>\n",
       "      <td>6041.52</td>\n",
       "      <td>761.58</td>\n",
       "      <td>453.21</td>\n",
       "      <td>144.97</td>\n",
       "      <td>13.33</td>\n",
       "      <td>1</td>\n",
       "      <td>37696.21</td>\n",
       "      <td>65.67</td>\n",
       "      <td>8.07</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>312</td>\n",
       "      <td>950.27</td>\n",
       "      <td>440.86</td>\n",
       "      <td>37</td>\n",
       "      <td>780000.0</td>\n",
       "      <td>41.43</td>\n",
       "      <td>7.03</td>\n",
       "      <td>3350.0</td>\n",
       "      <td>0.17</td>\n",
       "      <td>...</td>\n",
       "      <td>1320.04</td>\n",
       "      <td>710.63</td>\n",
       "      <td>512.54</td>\n",
       "      <td>109.16</td>\n",
       "      <td>2.58</td>\n",
       "      <td>0</td>\n",
       "      <td>29038.17</td>\n",
       "      <td>65.66</td>\n",
       "      <td>7.35</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>932</th>\n",
       "      <td>200</td>\n",
       "      <td>12</td>\n",
       "      <td>92.42</td>\n",
       "      <td>364.42</td>\n",
       "      <td>135</td>\n",
       "      <td>97200.0</td>\n",
       "      <td>59.42</td>\n",
       "      <td>10.34</td>\n",
       "      <td>884.0</td>\n",
       "      <td>0.17</td>\n",
       "      <td>...</td>\n",
       "      <td>381.84</td>\n",
       "      <td>254.56</td>\n",
       "      <td>84.85</td>\n",
       "      <td>146.97</td>\n",
       "      <td>4.50</td>\n",
       "      <td>0</td>\n",
       "      <td>2593.50</td>\n",
       "      <td>65.85</td>\n",
       "      <td>6.39</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>933</th>\n",
       "      <td>201</td>\n",
       "      <td>11</td>\n",
       "      <td>98.82</td>\n",
       "      <td>248.64</td>\n",
       "      <td>159</td>\n",
       "      <td>89100.0</td>\n",
       "      <td>59.64</td>\n",
       "      <td>10.18</td>\n",
       "      <td>831.0</td>\n",
       "      <td>0.17</td>\n",
       "      <td>...</td>\n",
       "      <td>284.60</td>\n",
       "      <td>180.00</td>\n",
       "      <td>150.00</td>\n",
       "      <td>51.96</td>\n",
       "      <td>1.90</td>\n",
       "      <td>0</td>\n",
       "      <td>4361.25</td>\n",
       "      <td>65.70</td>\n",
       "      <td>6.53</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>934</th>\n",
       "      <td>202</td>\n",
       "      <td>14</td>\n",
       "      <td>25.14</td>\n",
       "      <td>428.86</td>\n",
       "      <td>24</td>\n",
       "      <td>113400.0</td>\n",
       "      <td>60.14</td>\n",
       "      <td>17.94</td>\n",
       "      <td>847.0</td>\n",
       "      <td>0.30</td>\n",
       "      <td>...</td>\n",
       "      <td>402.49</td>\n",
       "      <td>180.00</td>\n",
       "      <td>180.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2.24</td>\n",
       "      <td>0</td>\n",
       "      <td>2153.05</td>\n",
       "      <td>65.91</td>\n",
       "      <td>6.12</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>935</th>\n",
       "      <td>203</td>\n",
       "      <td>10</td>\n",
       "      <td>96.00</td>\n",
       "      <td>451.30</td>\n",
       "      <td>68</td>\n",
       "      <td>81000.0</td>\n",
       "      <td>59.90</td>\n",
       "      <td>15.01</td>\n",
       "      <td>831.0</td>\n",
       "      <td>0.25</td>\n",
       "      <td>...</td>\n",
       "      <td>402.49</td>\n",
       "      <td>180.00</td>\n",
       "      <td>90.00</td>\n",
       "      <td>73.48</td>\n",
       "      <td>4.47</td>\n",
       "      <td>0</td>\n",
       "      <td>2421.43</td>\n",
       "      <td>65.97</td>\n",
       "      <td>6.32</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>936</th>\n",
       "      <td>204</td>\n",
       "      <td>11</td>\n",
       "      <td>7.73</td>\n",
       "      <td>235.73</td>\n",
       "      <td>135</td>\n",
       "      <td>89100.0</td>\n",
       "      <td>61.82</td>\n",
       "      <td>12.24</td>\n",
       "      <td>831.0</td>\n",
       "      <td>0.20</td>\n",
       "      <td>...</td>\n",
       "      <td>254.56</td>\n",
       "      <td>254.56</td>\n",
       "      <td>127.28</td>\n",
       "      <td>180.00</td>\n",
       "      <td>2.00</td>\n",
       "      <td>0</td>\n",
       "      <td>3782.68</td>\n",
       "      <td>65.65</td>\n",
       "      <td>6.26</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>937 rows × 49 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      0      1        2       3    4           5      6      7        8   \\\n",
       "0      1   2558  1506.09  456.63   90   6395000.0  40.88   7.89  29780.0   \n",
       "1      2  22325    79.11  841.03  180  55812500.0  51.11   1.21  61900.0   \n",
       "2      3    115  1449.85  608.43   88    287500.0  40.42   7.34   3340.0   \n",
       "3      4   1201  1562.53  295.65   66   3002500.0  42.40   7.97  18030.0   \n",
       "4      5    312   950.27  440.86   37    780000.0  41.43   7.03   3350.0   \n",
       "..   ...    ...      ...     ...  ...         ...    ...    ...      ...   \n",
       "932  200     12    92.42  364.42  135     97200.0  59.42  10.34    884.0   \n",
       "933  201     11    98.82  248.64  159     89100.0  59.64  10.18    831.0   \n",
       "934  202     14    25.14  428.86   24    113400.0  60.14  17.94    847.0   \n",
       "935  203     10    96.00  451.30   68     81000.0  59.90  15.01    831.0   \n",
       "936  204     11     7.73  235.73  135     89100.0  61.82  12.24    831.0   \n",
       "\n",
       "       9   ...       40        41       42       43     44  45        46  \\\n",
       "0    0.19  ...  2850.00   1000.00   763.16   135.46   3.73   0  33243.19   \n",
       "1    0.02  ...  5750.00  11500.00  9593.48  1648.80   0.60   0  51572.04   \n",
       "2    0.18  ...  1400.00    250.00   150.00    45.13   9.33   1  31692.84   \n",
       "3    0.19  ...  6041.52    761.58   453.21   144.97  13.33   1  37696.21   \n",
       "4    0.17  ...  1320.04    710.63   512.54   109.16   2.58   0  29038.17   \n",
       "..    ...  ...      ...       ...      ...      ...    ...  ..       ...   \n",
       "932  0.17  ...   381.84    254.56    84.85   146.97   4.50   0   2593.50   \n",
       "933  0.17  ...   284.60    180.00   150.00    51.96   1.90   0   4361.25   \n",
       "934  0.30  ...   402.49    180.00   180.00     0.00   2.24   0   2153.05   \n",
       "935  0.25  ...   402.49    180.00    90.00    73.48   4.47   0   2421.43   \n",
       "936  0.20  ...   254.56    254.56   127.28   180.00   2.00   0   3782.68   \n",
       "\n",
       "        47    48  49  \n",
       "0    65.74  7.95   1  \n",
       "1    65.73  6.26   0  \n",
       "2    65.81  7.84   1  \n",
       "3    65.67  8.07   1  \n",
       "4    65.66  7.35   0  \n",
       "..     ...   ...  ..  \n",
       "932  65.85  6.39   0  \n",
       "933  65.70  6.53   0  \n",
       "934  65.91  6.12   0  \n",
       "935  65.97  6.32   0  \n",
       "936  65.65  6.26   0  \n",
       "\n",
       "[937 rows x 49 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Consider Columns That Have Very Few Values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate the number of unique values for each variable as a percentage of the total number of rows in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0, 238, 25.4%\n",
      "1, 297, 31.7%\n",
      "2, 927, 98.9%\n",
      "3, 933, 99.6%\n",
      "4, 179, 19.1%\n",
      "5, 375, 40.0%\n",
      "6, 820, 87.5%\n",
      "7, 618, 66.0%\n",
      "8, 561, 59.9%\n",
      "9, 57, 6.1%\n",
      "10, 577, 61.6%\n",
      "11, 59, 6.3%\n",
      "12, 73, 7.8%\n",
      "13, 107, 11.4%\n",
      "14, 53, 5.7%\n",
      "15, 91, 9.7%\n",
      "16, 893, 95.3%\n",
      "17, 810, 86.4%\n",
      "18, 170, 18.1%\n",
      "19, 53, 5.7%\n",
      "20, 68, 7.3%\n",
      "21, 9, 1.0%\n",
      "23, 92, 9.8%\n",
      "24, 9, 1.0%\n",
      "25, 8, 0.9%\n",
      "26, 9, 1.0%\n",
      "27, 308, 32.9%\n",
      "28, 447, 47.7%\n",
      "29, 392, 41.8%\n",
      "30, 107, 11.4%\n",
      "31, 42, 4.5%\n",
      "32, 4, 0.4%\n",
      "33, 45, 4.8%\n",
      "34, 141, 15.0%\n",
      "35, 110, 11.7%\n",
      "36, 3, 0.3%\n",
      "37, 758, 80.9%\n",
      "38, 9, 1.0%\n",
      "39, 9, 1.0%\n",
      "40, 388, 41.4%\n",
      "41, 220, 23.5%\n",
      "42, 644, 68.7%\n",
      "43, 649, 69.3%\n",
      "44, 499, 53.3%\n",
      "45, 2, 0.2%\n",
      "46, 937, 100.0%\n",
      "47, 169, 18.0%\n",
      "48, 286, 30.5%\n",
      "49, 2, 0.2%\n"
     ]
    }
   ],
   "source": [
    "for i in df.columns:\n",
    "    unums = len(unique(df.loc[:,i]))\n",
    "    percentage = float(unums)/data.shape[0] * 100\n",
    "    print('%d, %d, %.1f%%' % (i, unums, percentage))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we can see that some columns have a very low percentage of unique values, such as below 1 percent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21, 9, 1.0%\n",
      "24, 9, 1.0%\n",
      "25, 8, 0.9%\n",
      "26, 9, 1.0%\n",
      "32, 4, 0.4%\n",
      "36, 3, 0.3%\n",
      "38, 9, 1.0%\n",
      "39, 9, 1.0%\n",
      "45, 2, 0.2%\n",
      "49, 2, 0.2%\n"
     ]
    }
   ],
   "source": [
    "for i in df.columns:\n",
    "    unums = len(unique(df.loc[:,i]))\n",
    "    percentage = float(unums)/data.shape[0] * 100\n",
    "    if(percentage <= 1):\n",
    "        print('%d, %d, %.1f%%' % (i, unums, percentage))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To delete these columns we can do the following"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "nuni = df.nunique()\n",
    "todelete = [i for i,v in enumerate(nuni) if(float(v)/df.shape[0]*100) < 1]\n",
    "df.drop(todelete,axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(937, 39)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method 2: Variance threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(937, 49) (937,)\n",
      "(937, 48)\n"
     ]
    }
   ],
   "source": [
    "from pandas import read_csv\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "path = 'https://raw.githubusercontent.com/jbrownlee/Datasets/master/oil-spill.csv'\n",
    "df = read_csv(path, header=None)\n",
    "X = df.iloc[:,:-1]\n",
    "y = df.iloc[:,-1]\n",
    "print(X.shape,y.shape)\n",
    "vt = VarianceThreshold()\n",
    "xsel = vt.fit_transform(X)\n",
    "print(xsel.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">Threshold=0.00, Features=48\n",
      ">Threshold=0.05, Features=37\n",
      ">Threshold=0.10, Features=36\n",
      ">Threshold=0.15, Features=35\n",
      ">Threshold=0.20, Features=35\n",
      ">Threshold=0.25, Features=35\n",
      ">Threshold=0.30, Features=35\n",
      ">Threshold=0.35, Features=35\n",
      ">Threshold=0.40, Features=35\n",
      ">Threshold=0.45, Features=33\n",
      ">Threshold=0.50, Features=31\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "path = 'https://raw.githubusercontent.com/jbrownlee/Datasets/master/oil-spill.csv'\n",
    "df = read_csv(path, header=None)\n",
    "X = df.iloc[:,:-1]\n",
    "y = df.iloc[:,-1]\n",
    "results = []\n",
    "thresholds = np.arange(0.0, 0.55, 0.05)\n",
    "for t in thresholds:\n",
    "    vt = VarianceThreshold(threshold = t)\n",
    "    xsel = vt.fit_transform(X)\n",
    "    n_features = xsel.shape[1]\n",
    "    print('>Threshold=%.2f, Features=%d' % (t, n_features))\n",
    "    results.append(n_features)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[48, 37, 36, 35, 35, 35, 35, 35, 35, 33, 31]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "pyplot.plot(thresholds, results)\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Identify Rows That Contain Duplicate Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "       0    1    2    3               4\n",
      "34   4.9  3.1  1.5  0.1     Iris-setosa\n",
      "37   4.9  3.1  1.5  0.1     Iris-setosa\n",
      "142  5.8  2.7  5.1  1.9  Iris-virginica\n"
     ]
    }
   ],
   "source": [
    "from pandas import read_csv\n",
    "# define the location of the dataset\n",
    "path = 'https://raw.githubusercontent.com/jbrownlee/Datasets/master/iris.csv'\n",
    "# load the dataset\n",
    "df = read_csv(path, header=None)\n",
    "# calculate duplicates\n",
    "dups = df.duplicated()\n",
    "print(dups.any())\n",
    "# list all duplicate rows\n",
    "print(df[dups])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Delete Rows That Contain Duplicate Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(150, 5)\n",
      "(147, 5)\n"
     ]
    }
   ],
   "source": [
    "from pandas import read_csv\n",
    "# define the location of the dataset\n",
    "path = 'https://raw.githubusercontent.com/jbrownlee/Datasets/master/iris.csv'\n",
    "# load the dataset\n",
    "df = read_csv(path, header=None)\n",
    "print(df.shape)\n",
    "# delete duplicate rows\n",
    "df.drop_duplicates(inplace=True)\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outlier Detection and removal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will generally define outliers as samples that are exceptionally far from the mainstream of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Standard Deviation Method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- If we know that the distribution of values in the sample is Gaussian or Gaussian-like, we can use the standard deviation of the sample as a cut-off for identifying outliers.\n",
    "- Three standard deviations from the mean is a common cut-off in practice for identifying outliers in a Gaussian or Gaussian-like distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Identified outliers: 29\n",
      "Non-outlier observations: 9971\n"
     ]
    }
   ],
   "source": [
    "from numpy.random import seed\n",
    "from numpy.random import randn\n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "# seed the random number generator\n",
    "seed(1)\n",
    "# generate univariate observations\n",
    "data = 5 * randn(10000) + 50\n",
    "# calculate summary statistics\n",
    "data_mean, data_std = mean(data), std(data)\n",
    "# identify outliers\n",
    "cut_off = data_std * 3\n",
    "lower, upper = data_mean - cut_off, data_mean + cut_off\n",
    "# identify outliers\n",
    "outliers = [x for x in data if x < lower or x > upper]\n",
    "print('Identified outliers: %d' % len(outliers))\n",
    "# remove outliers\n",
    "outliers_removed = [x for x in data if x >= lower and x <= upper]\n",
    "print('Non-outlier observations: %d' % len(outliers_removed))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Interquartile Range Method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The IQR is calculated as the difference between the 75th and the 25th percentiles of the data and defines the box in a box and whisker plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentiles: 25th=46.685, 75th=53.359, IQR=6.674\n",
      "Identified outliers: 81\n",
      "Non-outlier observations: 9919\n"
     ]
    }
   ],
   "source": [
    "from numpy.random import seed\n",
    "from numpy.random import randn\n",
    "from numpy import percentile\n",
    "# seed the random number generator\n",
    "seed(1)\n",
    "# generate univariate observations\n",
    "data = 5 * randn(10000) + 50\n",
    "# calculate interquartile range\n",
    "q25, q75 = percentile(data, 25), percentile(data, 75)\n",
    "iqr = q75 - q25\n",
    "print('Percentiles: 25th=%.3f, 75th=%.3f, IQR=%.3f' % (q25, q75, iqr))\n",
    "# calculate the outlier cutoff\n",
    "cut_off = iqr * 1.5\n",
    "lower, upper = q25 - cut_off, q75 + cut_off\n",
    "# identify outliers\n",
    "outliers = [x for x in data if x < lower or x > upper]\n",
    "print('Identified outliers: %d' % len(outliers))\n",
    "# remove outliers\n",
    "outliers_removed = [x for x in data if x >= lower and x <= upper]\n",
    "print('Non-outlier observations: %d' % len(outliers_removed))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Automatic Outlier Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In machine learning, an approach to tackling the problem of outlier detection is one-class classification.\n",
    "- One-Class Classification, or OCC for short, involves fitting a model on the “normal” data and predicting whether new data is normal or an outlier/anomaly.\n",
    "- The local outlier factor, or LOF for short, is a technique that attempts to harness the idea of nearest neighbors for outlier detection. Each example is assigned a scoring of how isolated or how likely it is to be outliers based on the size of its local neighborhood. Those examples with the largest score are more likely to be outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE: 3.417\n",
      "(506, 13) (506,)\n",
      "(339, 13) (167, 13) (339,) (167,)\n",
      "MAE: 3.417\n"
     ]
    }
   ],
   "source": [
    "from pandas import read_csv\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "# evaluate model on the raw dataset\n",
    "from pandas import read_csv\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "# load the dataset\n",
    "url = 'https://raw.githubusercontent.com/jbrownlee/Datasets/master/housing.csv'\n",
    "df = read_csv(url, header=None)\n",
    "# retrieve the array\n",
    "data = df.values\n",
    "# split into inpiut and output elements\n",
    "X, y = data[:, :-1], data[:, -1]\n",
    "# split into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=1)\n",
    "# fit the model\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "# evaluate the model\n",
    "yhat = model.predict(X_test)\n",
    "# evaluate predictions\n",
    "mae = mean_absolute_error(y_test, yhat)\n",
    "print('MAE: %.3f' % mae)\n",
    "# evaluate model on the raw dataset\n",
    "from pandas import read_csv\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "# load the dataset\n",
    "url = 'https://raw.githubusercontent.com/jbrownlee/Datasets/master/housing.csv'\n",
    "df = read_csv(url, header=None)\n",
    "# retrieve the array\n",
    "data = df.values\n",
    "# split into inpiut and output elements\n",
    "X, y = data[:, :-1], data[:, -1]\n",
    "# summarize the shape of the dataset\n",
    "print(X.shape, y.shape)\n",
    "# split into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=1)\n",
    "# summarize the shape of the train and test sets\n",
    "print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)\n",
    "# fit the model\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "# evaluate the model\n",
    "yhat = model.predict(X_test)\n",
    "# evaluate predictions\n",
    "mae = mean_absolute_error(y_test, yhat)\n",
    "print('MAE: %.3f' % mae)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The expectation is that the outliers are causing the linear regression model to learn a bias or skewed understanding of the problem, and that removing these outliers from the training set will allow a more effective model to be learned."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can achieve this by defining the LocalOutlierFactor model and using it to make a prediction on the training dataset, marking each row in the training dataset as normal (1) or an outlier (-1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(339, 13) (339,)\n",
      "[ 1  1  1  1  1 -1  1 -1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1\n",
      "  1  1  1  1  1  1  1  1 -1  1  1  1  1  1  1  1  1  1  1  1  1  1  1 -1\n",
      "  1 -1  1  1  1 -1  1  1  1  1  1  1  1  1  1  1 -1  1  1  1  1  1  1 -1\n",
      "  1  1  1  1  1 -1  1  1  1  1  1  1 -1  1  1  1  1  1  1  1  1  1  1  1\n",
      "  1  1  1  1  1 -1  1  1  1  1  1  1  1  1  1 -1  1  1  1  1  1  1 -1  1\n",
      "  1 -1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1 -1  1  1  1  1\n",
      "  1  1  1  1  1  1  1  1  1  1  1  1  1 -1  1  1  1  1  1  1  1  1  1  1\n",
      "  1  1  1  1  1  1  1  1  1  1  1 -1  1  1  1  1  1  1  1  1  1  1  1  1\n",
      " -1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1\n",
      "  1  1  1  1  1  1  1  1 -1  1  1  1  1  1  1  1  1  1  1  1 -1  1  1  1\n",
      "  1  1 -1  1  1  1  1  1  1  1  1  1  1 -1  1  1  1 -1  1  1 -1  1 -1  1\n",
      "  1  1  1 -1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1 -1  1 -1  1  1\n",
      "  1  1  1  1  1  1  1 -1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1 -1\n",
      " -1 -1  1  1  1  1  1  1  1  1  1  1  1  1 -1  1  1  1  1  1 -1  1  1  1\n",
      "  1  1  1]\n",
      "(305, 13) (305,)\n",
      "MAE: 3.356\n"
     ]
    }
   ],
   "source": [
    "from pandas import read_csv\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "# load the dataset\n",
    "url = 'https://raw.githubusercontent.com/jbrownlee/Datasets/master/housing.csv'\n",
    "df = read_csv(url, header=None)\n",
    "# retrieve the array\n",
    "data = df.values\n",
    "# split into inpiut and output elements\n",
    "X, y = data[:, :-1], data[:, -1]\n",
    "# split into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=1)\n",
    "# summarize the shape of the training dataset\n",
    "print(X_train.shape, y_train.shape)\n",
    "# identify outliers in the training dataset\n",
    "lof = LocalOutlierFactor()\n",
    "yhat = lof.fit_predict(X_train)\n",
    "print(yhat)\n",
    "# select all rows that are not outliers\n",
    "mask = yhat != -1\n",
    "X_train, y_train = X_train[mask, :], y_train[mask]\n",
    "# summarize the shape of the updated training dataset\n",
    "print(X_train.shape, y_train.shape)\n",
    "# fit the model\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "# evaluate the model\n",
    "yhat = model.predict(X_test)\n",
    "# evaluate predictions\n",
    "mae = mean_absolute_error(y_test, yhat)\n",
    "print('MAE: %.3f' % mae)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Isolation Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- A tree-based anomaly detection algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(339, 13) (339,)\n",
      "(305, 13) (305,)\n",
      "MAE: 3.290\n"
     ]
    }
   ],
   "source": [
    "from pandas import read_csv\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "# load the dataset\n",
    "url = 'https://raw.githubusercontent.com/jbrownlee/Datasets/master/housing.csv'\n",
    "df = read_csv(url, header=None)\n",
    "# retrieve the array\n",
    "data = df.values\n",
    "# split into input and output elements\n",
    "X, y = data[:, :-1], data[:, -1]\n",
    "# split into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=1)\n",
    "# summarize the shape of the training dataset\n",
    "print(X_train.shape, y_train.shape)\n",
    "# identify outliers in the training dataset\n",
    "iso = IsolationForest(contamination=0.1)\n",
    "yhat = iso.fit_predict(X_train)\n",
    "# select all rows that are not outliers\n",
    "mask = yhat != -1\n",
    "X_train, y_train = X_train[mask, :], y_train[mask]\n",
    "# summarize the shape of the updated training dataset\n",
    "print(X_train.shape, y_train.shape)\n",
    "# fit the model\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "# evaluate the model\n",
    "yhat = model.predict(X_test)\n",
    "# evaluate predictions\n",
    "mae = mean_absolute_error(y_test, yhat)\n",
    "print('MAE: %.3f' % mae)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Minimum Covariance Determinant"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This approach can be generalized by defining a hypersphere (ellipsoid) that covers the normal data, and data that falls outside this shape is considered an outlier. An efficient implementation of this technique for multivariate data is known as the Minimum Covariance Determinant, or MCD for short."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(339, 13) (339,)\n",
      "(335, 13) (335,)\n",
      "MAE: 3.388\n"
     ]
    }
   ],
   "source": [
    "from pandas import read_csv\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.covariance import EllipticEnvelope\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "# load the dataset\n",
    "url = 'https://raw.githubusercontent.com/jbrownlee/Datasets/master/housing.csv'\n",
    "df = read_csv(url, header=None)\n",
    "# retrieve the array\n",
    "data = df.values\n",
    "# split into input and output elements\n",
    "X, y = data[:, :-1], data[:, -1]\n",
    "# split into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=1)\n",
    "# summarize the shape of the training dataset\n",
    "print(X_train.shape, y_train.shape)\n",
    "# identify outliers in the training dataset\n",
    "ee = EllipticEnvelope(contamination=0.01)\n",
    "yhat = ee.fit_predict(X_train)\n",
    "# select all rows that are not outliers\n",
    "mask = yhat != -1\n",
    "X_train, y_train = X_train[mask, :], y_train[mask]\n",
    "# summarize the shape of the updated training dataset\n",
    "print(X_train.shape, y_train.shape)\n",
    "# fit the model\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "# evaluate the model\n",
    "yhat = model.predict(X_test)\n",
    "# evaluate predictions\n",
    "mae = mean_absolute_error(y_test, yhat)\n",
    "print('MAE: %.3f' % mae)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SimpleImputer and transform test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2.]\n"
     ]
    }
   ],
   "source": [
    "from numpy import nan\n",
    "from pandas import read_csv\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "url = 'https://raw.githubusercontent.com/jbrownlee/Datasets/master/horse-colic.csv'\n",
    "dataframe = read_csv(url, header=None, na_values='?')\n",
    "data = dataframe.values\n",
    "ix = [i for i in range(data.shape[1]) if i != 23]\n",
    "X, y = data[:, ix], data[:, 23]\n",
    "\n",
    "\n",
    "row = [2,1,530101,38.50,66,28,3,3,nan,2,5,4,4,nan,nan,nan,3,5,45.00,8.40,nan,nan,2,11300,0,0,2]\n",
    "pipe = Pipeline(steps=[('i', SimpleImputer(strategy='constant')), ('m', RandomForestClassifier())])\n",
    "pipe.fit(X, y)\n",
    "yhat = pipe.predict([row])\n",
    "print(yhat)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNNImputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Accuracy: 0.860 (0.055)\n"
     ]
    }
   ],
   "source": [
    "from numpy import mean\n",
    "from numpy import std\n",
    "from pandas import read_csv\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.pipeline import Pipeline\n",
    "# load dataset\n",
    "url = 'https://raw.githubusercontent.com/jbrownlee/Datasets/master/horse-colic.csv'\n",
    "dataframe = read_csv(url, header=None, na_values='?')\n",
    "# split into input and output elements\n",
    "data = dataframe.values\n",
    "ix = [i for i in range(data.shape[1]) if i != 23]\n",
    "X, y = data[:, ix], data[:, 23]\n",
    "# define modeling pipeline\n",
    "model = RandomForestClassifier()\n",
    "imputer = KNNImputer()\n",
    "pipeline = Pipeline(steps=[('i', imputer), ('m', model)])\n",
    "# define model evaluation\n",
    "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "# evaluate model\n",
    "scores = cross_val_score(pipeline, X, y, scoring='accuracy', cv=cv, n_jobs=-1, error_score='raise')\n",
    "print('Mean Accuracy: %.3f (%.3f)' % (mean(scores), std(scores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Class: 2\n"
     ]
    }
   ],
   "source": [
    "pipeline.fit(X, y)\n",
    "row = [2, 1, 530101, 38.50, 66, 28, 3, 3, nan, 2, 5, 4, 4, nan, nan, nan, 3, 5, 45.00, 8.40, nan, nan, 2, 11300, 0,0, 2]\n",
    "# make a prediction\n",
    "yhat = pipeline.predict([row])\n",
    "# summarize prediction\n",
    "print('Predicted Class: %d' % yhat[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Iterative imputation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Iterative imputation refers to a process where each feature is modeled as a function of the other features, e.g. a regression problem where missing values are predicted. Each feature is imputed sequentially, one after the other, allowing prior imputed values to be used as part of a model in predicting subsequent features.\n",
    "- Different regression algorithms can be used to estimate the missing values for each feature, although linear methods are often used for simplicity. The number of iterations of the procedure is often kept small, such as 10. Finally, the order that features are processed sequentially can be considered, such as from the feature with the least missing values to the feature with the most missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Class: 2\n"
     ]
    }
   ],
   "source": [
    "from numpy import nan\n",
    "from pandas import read_csv\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "# load dataset\n",
    "url = 'https://raw.githubusercontent.com/jbrownlee/Datasets/master/horse-colic.csv'\n",
    "dataframe = read_csv(url, header=None, na_values='?')\n",
    "# split into input and output elements\n",
    "data = dataframe.values\n",
    "X, y = data[:, :-1], data[:, -1]\n",
    "# create the modeling pipeline\n",
    "pipeline = Pipeline(steps=[('i', IterativeImputer()), ('m', RandomForestClassifier())])\n",
    "# fit the model\n",
    "pipeline.fit(X, y)\n",
    "# define new data\n",
    "row = [2, 1, 530101, 38.50, 66, 28, 3, 3, nan, 2, 5, 4, 4, nan, nan, nan, 3, 5, 45.00, 8.40, nan, nan, 2, 11300, 00000, 00000, 2]\n",
    "# make a prediction\n",
    "yhat = pipeline.predict([row])\n",
    "# summarize prediction\n",
    "print('Predicted Class: %d' % yhat[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TensorFlow",
   "language": "python",
   "name": "tensorflowenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
