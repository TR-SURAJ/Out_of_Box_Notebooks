{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Feature selection provides an alternative to random subspaces for selecting groups of input features\n",
    "- One Method: Generate a feature subspace for each number of features from 1 to the number of columns in the dataset, fit a model on each, and combine their predictions\n",
    "- Multiple Methods: Generate a feature subspace using multiple different feature selection methods, fit a model on each, and combine their predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Suraj\\anaconda3\\envs\\tensorflowenv\\lib\\site-packages\\numpy\\_distributor_init.py:32: UserWarning: loaded more than 1 DLL from .libs:\n",
      "C:\\Users\\Suraj\\anaconda3\\envs\\tensorflowenv\\lib\\site-packages\\numpy\\.libs\\libopenblas.PYQHXLVVQ7VESDPUVUADXEVJOBGHJPAY.gfortran-win_amd64.dll\n",
      "C:\\Users\\Suraj\\anaconda3\\envs\\tensorflowenv\\lib\\site-packages\\numpy\\.libs\\libopenblas.WCDJNK7YVMPZQ2ME2ZZHJJRJ3JIKNDB7.gfortran-win_amd64.dll\n",
      "  stacklevel=1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Accuracy: 0.801 (0.053)\n"
     ]
    }
   ],
   "source": [
    "# evaluate a decision tree on the classification dataset\n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "# define dataset\n",
    "X, y = make_classification(n_samples=1000, n_features=20, n_informative=15, n_redundant=5, random_state=5)\n",
    "# define the random subspace ensemble model\n",
    "model = DecisionTreeClassifier()\n",
    "# define the evaluation method\n",
    "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "# evaluate the model on the dataset\n",
    "n_scores = cross_val_score(model, X, y, scoring='accuracy', cv=cv, n_jobs=-1)\n",
    "# report performance\n",
    "print('Mean Accuracy: %.3f (%.3f)' % (mean(n_scores), std(n_scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Single Feature Selection Method Ensembles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Creating an ensemble from the features selected by individual feature selection methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Accuracy: 0.828 (0.042)\n"
     ]
    }
   ],
   "source": [
    "#ANOVA F-statistic Ensemble\n",
    "# example of an ensemble created from features selected with the anova f-statistic\n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import f_classif\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from matplotlib import pyplot\n",
    "\n",
    "# get a voting ensemble of models\n",
    "def get_ensemble(n_features):\n",
    "    # define the base models\n",
    "    models = list()\n",
    "    # enumerate the features in the training dataset\n",
    "    for i in range(1, n_features+1):\n",
    "        # create the feature selection transform\n",
    "        fs = SelectKBest(score_func=f_classif, k=i)\n",
    "        # create the model\n",
    "        model = DecisionTreeClassifier()\n",
    "        # create the pipeline\n",
    "        pipe = Pipeline([('fs',fs), ('m', model)])\n",
    "        # add as a tuple to the list of models for voting\n",
    "        models.append((str(i),pipe))\n",
    "    # define the voting ensemble\n",
    "    ensemble = VotingClassifier(estimators=models, voting='hard')\n",
    "    return ensemble\n",
    "\n",
    "# define dataset\n",
    "X, y = make_classification(n_samples=1000, n_features=20, n_informative=15, n_redundant=5, random_state=5)\n",
    "# get the ensemble model\n",
    "ensemble = get_ensemble(X.shape[1])\n",
    "# define the evaluation method\n",
    "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "# evaluate the model on the dataset\n",
    "n_scores = cross_val_score(ensemble, X, y, scoring='accuracy', cv=cv, n_jobs=-1)\n",
    "# report performance\n",
    "print('Mean Accuracy: %.3f (%.3f)' % (mean(n_scores), std(n_scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can see improvement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mutual Information Ensemble"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Mutual information is calculated between two variables and measures the reduction in uncertainty for one variable given a known value of the other variable. It is straightforward when considering the distribution of two discrete (categorical or ordinal) variables, such as categorical input and categorical output data. Nevertheless, it can be adapted for use with numerical input and categorical output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Accuracy: 0.831 (0.041)\n"
     ]
    }
   ],
   "source": [
    "# example of an ensemble created from features selected with mutual information\n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from matplotlib import pyplot\n",
    "\n",
    "# get a voting ensemble of models\n",
    "def get_ensemble(n_features):\n",
    "    # define the base models\n",
    "    models = list()\n",
    "    # enumerate the features in the training dataset\n",
    "    for i in range(1, n_features+1):\n",
    "        # create the feature selection transform\n",
    "        fs = SelectKBest(score_func=mutual_info_classif, k=i)\n",
    "        # create the model\n",
    "        model = DecisionTreeClassifier()\n",
    "        # create the pipeline\n",
    "        pipe = Pipeline([('fs',fs), ('m', model)])\n",
    "        # add as a tuple to the list of models for voting\n",
    "        models.append((str(i),pipe))\n",
    "    # define the voting ensemble\n",
    "    ensemble = VotingClassifier(estimators=models, voting='hard')\n",
    "    return ensemble\n",
    "\n",
    "# define dataset\n",
    "X, y = make_classification(n_samples=1000, n_features=20, n_informative=15, n_redundant=5, random_state=5)\n",
    "# get the ensemble model\n",
    "ensemble = get_ensemble(X.shape[1])\n",
    "# define the evaluation method\n",
    "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "# evaluate the model on the dataset\n",
    "n_scores = cross_val_score(ensemble, X, y, scoring='accuracy', cv=cv, n_jobs=-1)\n",
    "# report performance\n",
    "print('Mean Accuracy: %.3f (%.3f)' % (mean(n_scores), std(n_scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Recursive Feature Selection Ensemble"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recursive Feature Elimination, or RFE for short, works by searching for a subset of features by starting with all features in the training dataset and successfully removing features until the desired number remains.\n",
    "\n",
    "This is achieved by fitting the given machine learning algorithm used in the core of the model, ranking features by importance, discarding the least important features, and re-fitting the model. This process is repeated until a specified number of features remains."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Accuracy: 0.822 (0.040)\n"
     ]
    }
   ],
   "source": [
    "# example of an ensemble created from features selected with RFE\n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from matplotlib import pyplot\n",
    "\n",
    "# get a voting ensemble of models\n",
    "def get_ensemble(n_features):\n",
    "    # define the base models\n",
    "    models = list()\n",
    "    # enumerate the features in the training dataset\n",
    "    for i in range(1, n_features+1):\n",
    "        # create the feature selection transform\n",
    "        fs = RFE(estimator=DecisionTreeClassifier(), n_features_to_select=i)\n",
    "        # create the model\n",
    "        model = DecisionTreeClassifier()\n",
    "        # create the pipeline\n",
    "        pipe = Pipeline([('fs',fs), ('m', model)])\n",
    "        # add as a tuple to the list of models for voting\n",
    "        models.append((str(i),pipe))\n",
    "    # define the voting ensemble\n",
    "    ensemble = VotingClassifier(estimators=models, voting='hard')\n",
    "    return ensemble\n",
    "\n",
    "# define dataset\n",
    "X, y = make_classification(n_samples=1000, n_features=20, n_informative=15, n_redundant=5, random_state=5)\n",
    "# get the ensemble model\n",
    "ensemble = get_ensemble(X.shape[1])\n",
    "# define the evaluation method\n",
    "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "# evaluate the model on the dataset\n",
    "n_scores = cross_val_score(ensemble, X, y, scoring='accuracy', cv=cv, n_jobs=-1)\n",
    "# report performance\n",
    "print('Mean Accuracy: %.3f (%.3f)' % (mean(n_scores), std(n_scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ensemble With Fixed Number of Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Accuracy: 0.846 (0.036)\n"
     ]
    }
   ],
   "source": [
    "from numpy import mean\n",
    "from numpy import std\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "from sklearn.feature_selection import f_classif\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from matplotlib import pyplot\n",
    " \n",
    "# get a voting ensemble of models\n",
    "def get_ensemble(n_features):\n",
    "    # define the base models\n",
    "    models = list()\n",
    "    # anova\n",
    "    fs = SelectKBest(score_func=f_classif, k=n_features)\n",
    "    anova = Pipeline([('fs', fs), ('m', DecisionTreeClassifier())])\n",
    "    models.append(('anova', anova))\n",
    "    # mutual information\n",
    "    fs = SelectKBest(score_func=mutual_info_classif, k=n_features)\n",
    "    mutinfo = Pipeline([('fs', fs), ('m', DecisionTreeClassifier())])\n",
    "    models.append(('mutinfo', mutinfo))\n",
    "    # rfe\n",
    "    fs = RFE(estimator=DecisionTreeClassifier(), n_features_to_select=n_features)\n",
    "    rfe = Pipeline([('fs', fs), ('m', DecisionTreeClassifier())])\n",
    "    models.append(('rfe', rfe))\n",
    "    # define the voting ensemble\n",
    "    ensemble = VotingClassifier(estimators=models, voting='hard')\n",
    "    return ensemble\n",
    " \n",
    "# define dataset\n",
    "X, y = make_classification(n_samples=1000, n_features=20, n_informative=15, n_redundant=5, random_state=1)\n",
    "# get the ensemble model\n",
    "ensemble = get_ensemble(15)\n",
    "# define the evaluation method\n",
    "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "# evaluate the model on the dataset\n",
    "n_scores = cross_val_score(ensemble, X, y, scoring='accuracy', cv=cv, n_jobs=-1)\n",
    "# report performance\n",
    "print('Mean Accuracy: %.3f (%.3f)' % (mean(n_scores), std(n_scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Best till now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">anova: 0.823 (0.041)\n",
      ">mutinfo: 0.810 (0.044)\n",
      ">rfe: 0.824 (0.039)\n",
      ">ensemble: 0.837 (0.043)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# comparison of ensemble of a fixed number features to single models fit on each set of features\n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "from sklearn.feature_selection import f_classif\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from matplotlib import pyplot\n",
    "\n",
    "# get a voting ensemble of models\n",
    "def get_ensemble(n_features):\n",
    "    # define the base models\n",
    "    models, names = list(), list()\n",
    "    # anova\n",
    "    fs = SelectKBest(score_func=f_classif, k=n_features)\n",
    "    anova = Pipeline([('fs', fs), ('m', DecisionTreeClassifier())])\n",
    "    models.append(('anova', anova))\n",
    "    names.append('anova')\n",
    "    # mutual information\n",
    "    fs = SelectKBest(score_func=mutual_info_classif, k=n_features)\n",
    "    mutinfo = Pipeline([('fs', fs), ('m', DecisionTreeClassifier())])\n",
    "    models.append(('mutinfo', mutinfo))\n",
    "    names.append('mutinfo')\n",
    "    # rfe\n",
    "    fs = RFE(estimator=DecisionTreeClassifier(), n_features_to_select=n_features)\n",
    "    rfe = Pipeline([('fs', fs), ('m', DecisionTreeClassifier())])\n",
    "    models.append(('rfe', rfe))\n",
    "    names.append('rfe')\n",
    "    # define the voting ensemble\n",
    "    ensemble = VotingClassifier(estimators=models, voting='hard')\n",
    "    names.append('ensemble')\n",
    "    return names, [anova, mutinfo, rfe, ensemble]\n",
    "\n",
    "# define dataset\n",
    "X, y = make_classification(n_samples=1000, n_features=20, n_informative=15, n_redundant=5, random_state=1)\n",
    "# get the ensemble model\n",
    "names, models = get_ensemble(15)\n",
    "# evaluate each model\n",
    "results = list()\n",
    "for model,name in zip(models,names):\n",
    "    # define the evaluation method\n",
    "    cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "    # evaluate the model on the dataset\n",
    "    n_scores = cross_val_score(model, X, y, scoring='accuracy', cv=cv, n_jobs=-1)\n",
    "    # report performance\n",
    "    print('>%s: %.3f (%.3f)' % (name, mean(n_scores), std(n_scores)))\n",
    "    results.append(n_scores)\n",
    "# plot the results for comparison\n",
    "pyplot.boxplot(results, labels=names)\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ensemble outperforms individual models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ensemble With Contiguous Number of Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Accuracy: 0.861 (0.036)\n"
     ]
    }
   ],
   "source": [
    "# ensemble of many subsets of features selected by multiple feature selection methods\n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "from sklearn.feature_selection import f_classif\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from matplotlib import pyplot\n",
    "\n",
    "# get a voting ensemble of models\n",
    "def get_ensemble(n_features_start, n_features_end):\n",
    "    # define the base models\n",
    "    models = list()\n",
    "    for i in range(n_features_start, n_features_end+1):\n",
    "        # anova\n",
    "        fs = SelectKBest(score_func=f_classif, k=i)\n",
    "        anova = Pipeline([('fs', fs), ('m', DecisionTreeClassifier())])\n",
    "        models.append(('anova'+str(i), anova))\n",
    "        # mutual information\n",
    "        fs = SelectKBest(score_func=mutual_info_classif, k=i)\n",
    "        mutinfo = Pipeline([('fs', fs), ('m', DecisionTreeClassifier())])\n",
    "        models.append(('mutinfo'+str(i), mutinfo))\n",
    "        # rfe\n",
    "        fs = RFE(estimator=DecisionTreeClassifier(), n_features_to_select=i)\n",
    "        rfe = Pipeline([('fs', fs), ('m', DecisionTreeClassifier())])\n",
    "        models.append(('rfe'+str(i), rfe))\n",
    "    # define the voting ensemble\n",
    "    ensemble = VotingClassifier(estimators=models, voting='hard')\n",
    "    return ensemble\n",
    "\n",
    "# define dataset\n",
    "X, y = make_classification(n_samples=1000, n_features=20, n_informative=15, n_redundant=5, random_state=1)\n",
    "# get the ensemble model\n",
    "ensemble = get_ensemble(1, 20)\n",
    "# define the evaluation method\n",
    "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "# evaluate the model on the dataset\n",
    "n_scores = cross_val_score(ensemble, X, y, scoring='accuracy', cv=cv, n_jobs=-1)\n",
    "# report performance\n",
    "print('Mean Accuracy: %.3f (%.3f)' % (mean(n_scores), std(n_scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extra Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Extremely Randomized Trees, or Extra Trees for short, is an ensemble machine learning algorithm\n",
    "- Specifically, it is an ensemble of decision trees and is related to other ensembles of decision trees algorithms such as bootstrap aggregation (bagging) and random forest\n",
    "- The Extra Trees algorithm works by creating a large number of unpruned decision trees from the training dataset. Predictions are made by averaging the prediction of the decision trees in the case of regression or using majority voting in the case of classification\n",
    "- Unlike bagging and random forest that develop each decision tree from a bootstrap sample of the training dataset, the Extra Trees algorithm fits each decision tree on the whole training dataset\n",
    "- Like random forest, the Extra Trees algorithm will randomly sample the features at each split point of a decision tree\n",
    "- Unlike random forest, which uses a greedy algorithm to select an optimal split point, the Extra Trees algorithm selects a split point at random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.911 (0.024)\n"
     ]
    }
   ],
   "source": [
    "# evaluate extra trees algorithm for classification\n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "# define dataset\n",
    "X, y = make_classification(n_samples=1000, n_features=20, n_informative=15, n_redundant=5, random_state=4)\n",
    "# define the model\n",
    "model = ExtraTreesClassifier()\n",
    "# evaluate the model\n",
    "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "n_scores = cross_val_score(model, X, y, scoring='accuracy', cv=cv, n_jobs=-1, error_score='raise')\n",
    "# report performance\n",
    "print('Accuracy: %.3f (%.3f)' % (mean(n_scores), std(n_scores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Class: 0\n"
     ]
    }
   ],
   "source": [
    "# make predictions using extra trees for classification\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "# define dataset\n",
    "X, y = make_classification(n_samples=1000, n_features=20, n_informative=15, n_redundant=5, random_state=4)\n",
    "# define the model\n",
    "model = ExtraTreesClassifier()\n",
    "# fit the model on the whole dataset\n",
    "model.fit(X, y)\n",
    "# make a single prediction\n",
    "row = [[-3.52169364,4.00560592,2.94756812,-0.09755101,-0.98835896,1.81021933,-0.32657994,1.08451928,4.98150546,-2.53855736,3.43500614,1.64660497,-4.1557091,-1.55301045,-0.30690987,-1.47665577,6.818756,0.5132918,4.3598337,-4.31785495]]\n",
    "yhat = model.predict(row)\n",
    "print('Predicted Class: %d' % yhat[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extra Trees for Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE: -69.352 (5.330)\n"
     ]
    }
   ],
   "source": [
    "# evaluate extra trees ensemble for regression\n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RepeatedKFold\n",
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "# define dataset\n",
    "X, y = make_regression(n_samples=1000, n_features=20, n_informative=15, noise=0.1, random_state=3)\n",
    "# define the model\n",
    "model = ExtraTreesRegressor()\n",
    "# evaluate the model\n",
    "cv = RepeatedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "n_scores = cross_val_score(model, X, y, scoring='neg_mean_absolute_error', cv=cv, n_jobs=-1, error_score='raise')\n",
    "# report performance\n",
    "print('MAE: %.3f (%.3f)' % (mean(n_scores), std(n_scores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: 53\n"
     ]
    }
   ],
   "source": [
    "# extra trees for making predictions for regression\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "# define dataset\n",
    "X, y = make_regression(n_samples=1000, n_features=20, n_informative=15, noise=0.1, random_state=3)\n",
    "# define the model\n",
    "model = ExtraTreesRegressor()\n",
    "# fit the model on the whole dataset\n",
    "model.fit(X, y)\n",
    "# make a single prediction\n",
    "row = [[-0.56996683,0.80144889,2.77523539,1.32554027,-1.44494378,-0.80834175,-0.84142896,0.57710245,0.96235932,-0.66303907,-1.13994112,0.49887995,1.40752035,-0.2995842,-0.05708706,-2.08701456,1.17768469,0.13474234,0.09518152,-0.07603207]]\n",
    "yhat = model.predict(row)\n",
    "print('Prediction: %d' % yhat[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bagging-ensemble-with-different-data-transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Accuracy: 0.833 (0.043)\n"
     ]
    }
   ],
   "source": [
    "# evaluate data transform bagging ensemble on a classification dataset\n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.preprocessing import PowerTransformer\n",
    "from sklearn.preprocessing import QuantileTransformer\n",
    "from sklearn.preprocessing import KBinsDiscretizer\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# get a voting ensemble of models\n",
    "def get_ensemble():\n",
    "    # define the base models\n",
    "    models = list()\n",
    "    # normalization\n",
    "    norm = Pipeline([('s', MinMaxScaler()), ('m', DecisionTreeClassifier())])\n",
    "    models.append(('norm', norm))\n",
    "    # standardization\n",
    "    std = Pipeline([('s', StandardScaler()), ('m', DecisionTreeClassifier())])\n",
    "    models.append(('std', std))\n",
    "    # robust\n",
    "    robust = Pipeline([('s', RobustScaler()), ('m', DecisionTreeClassifier())])\n",
    "    models.append(('robust', robust))\n",
    "    # power\n",
    "    power = Pipeline([('s', PowerTransformer()), ('m', DecisionTreeClassifier())])\n",
    "    models.append(('power', power))\n",
    "    # quantile\n",
    "    quant = Pipeline([('s', QuantileTransformer(n_quantiles=100, output_distribution='normal')), ('m', DecisionTreeClassifier())])\n",
    "    models.append(('quant', quant))\n",
    "    # kbins\n",
    "    kbins = Pipeline([('s', KBinsDiscretizer(n_bins=20, encode='ordinal')), ('m', DecisionTreeClassifier())])\n",
    "    models.append(('kbins', kbins))\n",
    "    # define the voting ensemble\n",
    "    ensemble = VotingClassifier(estimators=models, voting='hard')\n",
    "    return ensemble\n",
    "\n",
    "# define dataset\n",
    "X, y = make_classification(n_samples=1000, n_features=20, n_informative=15, n_redundant=5, random_state=1)\n",
    "# get models\n",
    "ensemble = get_ensemble()\n",
    "# define the evaluation procedure\n",
    "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "# evaluate the model\n",
    "n_scores = cross_val_score(ensemble, X, y, scoring='accuracy', cv=cv, n_jobs=-1)\n",
    "# report performance\n",
    "print('Mean Accuracy: %.3f (%.3f)' % (mean(n_scores), std(n_scores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TensorFlow",
   "language": "python",
   "name": "tensorflowenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
